{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from loadData import loadData, convertToBatches\n",
    "inputDir = \"GEFCom2012/\"\n",
    "\n",
    "nHorizons = 24\n",
    "timeSteps = 20\n",
    "batchSize = 20\n",
    "\n",
    "trainingDfs, completeDfs = loadData(\"GEFCom2012/\", maxDataPoints = -1)\n",
    "ts = trainingDfs[0][[\"zone.1\"]].values\n",
    "batches = convertToBatches(ts, timeSteps, batchSize, nHorizons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "    \"\"\"Recursive Neural Network\"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, maxGradient, batchSize, timeSteps, nHorizons, inputSize, nHiddenUnits, nLayers):\n",
    "        self.maxGradient = maxGradient\n",
    "        self.nLayers = nLayers\n",
    "        \n",
    "        with tf.name_scope(\"Parameters\"):\n",
    "            self.learning_rate = tf.placeholder(tf.float32, name=\"learningRate\")\n",
    "            self.keep_probability = tf.placeholder(tf.float32, name=\"keepProbability\")\n",
    "\n",
    "        with tf.name_scope(\"Input\"):\n",
    "            self.input = tf.placeholder(tf.float32, shape=(batchSize, timeSteps), name=\"input\")\n",
    "            self.targets = tf.placeholder(tf.float32, shape=(batchSize, timeSteps, nHorizons), name=\"targets\")\n",
    "            self.init = tf.placeholder(tf.float32, shape=(), name=\"init\")\n",
    "        #Declare the CNN structure here!\n",
    "        #with tf.name_scope(\"Embedding\"):\n",
    "        #    self.embedding = tf.Variable(tf.random_uniform((inputSize, hidden_units), -self.init, self.init),\n",
    "        #                                 dtype=tf.float32,\n",
    "        #                                 name=\"embedding\")\n",
    "        #    self.w = tf.get_variable(\"w\", (inputSize, hidden_units))\n",
    "        #    self.b = tf.get_variable(\"b\", inputSize)\n",
    "            \n",
    "        #    self.embedded_input = tf.matmul(self.input, self.w) + self.b\n",
    "\n",
    "        with tf.name_scope(\"RNN\"):\n",
    "            cell = tf.nn.rnn_cell.LSTMCell(nHiddenUnits)\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=self.keepProbability)\n",
    "            rnn_layers = tf.nn.rnn_cell.MultiRNNCell([cell] * nLayers)\n",
    "            self.reset_state = rnn_layers.zero_state(batchSize, dtype=tf.float32)\n",
    "            self.state = tf.placeholder(tf.float32, self.reset_state.get_shape(), \"state\")\n",
    "            self.outputs, self.next_state = tf.nn.dynamic_rnn(rnn_layers, self.input, time_major=True,\n",
    "                                                              initial_state=self.state)\n",
    "\n",
    "        with tf.name_scope(\"Cost\"):\n",
    "            # Concatenate all the batches into a single row.\n",
    "            self.flattened_outputs = tf.reshape(tf.concat(1, self.outputs), (-1, hidden_units),\n",
    "                                                name=\"flattened_outputs\")\n",
    "            # Project the outputs onto the vocabulary.\n",
    "            self.w = tf.get_variable(\"w\", (hidden_units, nHorizons))\n",
    "            self.b = tf.get_variable(\"b\", nHorizons)\n",
    "            self.predicted = tf.matmul(self.flattened_outputs, self.w) + self.b\n",
    "            # Compare predictions to labels.\n",
    "            self.loss = tf.sqrt(tf.reduce_mean(tf.square(tf.sub(targets, outputs))))\n",
    "            self.cost = tf.div(tf.reduce_sum(self.loss), batch_size, name=\"cost\")\n",
    "\n",
    "        with tf.name_scope(\"Train\"):\n",
    "            #self.validation_perplexity = tf.Variable(dtype=tf.float32, initial_value=float(\"inf\"), trainable=False,\n",
    "            #                                         name=\"validation_perplexity\")\n",
    "            #tf.scalar_summary(self.validation_perplexity.op.name, self.validation_perplexity)\n",
    "            #self.training_epoch_perplexity = tf.Variable(dtype=tf.float32, initial_value=float(\"inf\"), trainable=False,\n",
    "            #                                             name=\"training_epoch_perplexity\")\n",
    "            #tf.scalar_summary(self.training_epoch_perplexity.op.name, self.training_epoch_perplexity)\n",
    "            self.iteration = tf.Variable(0, dtype=tf.int64, name=\"iteration\", trainable=False)\n",
    "            self.gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()),\n",
    "                                                       max_gradient, name=\"clipGradients\")\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learningRate)\n",
    "            self.trainStep = optimizer.apply_gradients(zip(self.gradients, tf.trainable_variables()),\n",
    "                                                        name=\"trainStep\",\n",
    "                                                        global_step=self.iteration)\n",
    "\n",
    "        self.initialize = tf.initialize_all_variables()\n",
    "        self.summary = tf.merge_all_summaries()\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self.input.get_shape()[0].value\n",
    "\n",
    "    @property\n",
    "    def time_steps(self):\n",
    "        return self.input.get_shape()[1].value\n",
    "\n",
    "    @property\n",
    "    def vocabulary_size(self):\n",
    "        return self.embedding.get_shape()[0].value\n",
    "\n",
    "    @property\n",
    "    def hidden_units(self):\n",
    "        return self.embedding.get_shape()[1].value\n",
    "\n",
    "    def train(self, session, init, ts, parameters, exitCriteria, validation, loggingInterval, Directories):\n",
    "        epoch = 1\n",
    "        iteration = 0\n",
    "        state = None\n",
    "        trainingSet = convertToBatches(ts, self.timeSteps, self.batchSize, self.nHorizons)\n",
    "        #summary = self.summary_writer(directories.summary, session)\n",
    "        session.run(self.initialize, feed_dict={self.init: init})\n",
    "        try:\n",
    "            # Enumerate over the training set until exit criteria are met.\n",
    "            while True:\n",
    "                epoch_cost = epoch_iteration = 0\n",
    "                #Reset state after every epoch\n",
    "                state = session.run(self.reset_state)\n",
    "                # Enumerate over a single epoch of the training set.\n",
    "                for xs, ys in trainingSet:\n",
    "                    _, cost, state, iteration = session.run(\n",
    "                        [self.train_step, self.cost, self.nextState, self.iteration],\n",
    "                        feed_dict={\n",
    "                            self.input: xs,\n",
    "                            self.targets: ys,\n",
    "                            self.state: state,\n",
    "                            self.learningRate: parameters.learningRate,\n",
    "                            self.keepProbability: parameters.keepProbability\n",
    "                        })\n",
    "                    epochCost += cost\n",
    "                    epochIteration += self.timeSteps\n",
    "                    if self._interval(iteration, logging_interval):\n",
    "                        logger.info(\"Epoch %d, Iteration %d: epoch training perplexity %0.4f\" %\n",
    "                                    (epoch, iteration, self.perplexity(epochCost, epochIteration)))\n",
    "                    #if validation is not None and self._interval(iteration, validation.interval):\n",
    "                    #    validation_perplexity = self.test(session, validation.validation_set)\n",
    "                    #    self.store_validation_perplexity(session, summary, iteration, validation_perplexity)\n",
    "                    #    logger.info(\"Epoch %d, Iteration %d: validation perplexity %0.4f\" %\n",
    "                    #                (epoch, iteration, validation_perplexity))\n",
    "                    if exitCriteria.maxIterations is not None and iteration > exitCriteria.maxIterations:\n",
    "                        raise StopTrainingException()\n",
    "\n",
    "                #self.store_training_epoch_perplexity(session, summary, iteration,\n",
    "                #                                     self.perplexity(epoch_cost, epoch_iteration))\n",
    "                epoch += 1\n",
    "                if exitCriteria.maxIterations is not None and iteration > exitCriteria.maxIterations:\n",
    "                        raise StopTrainingException()\n",
    "        except (StopTrainingException, KeyboardInterrupt):\n",
    "            pass\n",
    "        logger.info(\"Stop training at epoch %d, iteration %d\" % (epoch, iteration))\n",
    "        summary.close()\n",
    "        if directories.model is not None:\n",
    "            model_filename = self._model_file(directories.model)\n",
    "            tf.train.Saver().save(session, model_filename)\n",
    "            self._write_model_parameters(directories.model)\n",
    "            logger.info(\"Saved model in %s \" % directories.model)\n",
    "\n",
    "    def _write_model_parameters(self, model_directory):\n",
    "        parameters = {\n",
    "            \"max_gradient\": self.max_gradient,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"time_steps\": self.time_steps,\n",
    "            \"vocabulary_size\": self.vocabulary_size,\n",
    "            \"hidden_units\": self.hidden_units,\n",
    "            \"layers\": self.layers\n",
    "        }\n",
    "        with open(self._parameters_file(model_directory), \"w\") as f:\n",
    "            json.dump(parameters, f, indent=4)\n",
    "\n",
    "    def test(self, session, test_set):\n",
    "        state = None\n",
    "        epoch_cost = epoch_iteration = 0\n",
    "        for start_document, context, target, _ in test_set.epoch(self.time_steps, self.batch_size):\n",
    "            if start_document:\n",
    "                state = session.run(self.reset_state)\n",
    "            cost, state = session.run([self.cost, self.next_state],\n",
    "                                      feed_dict={\n",
    "                                          self.input: context,\n",
    "                                          self.targets: target,\n",
    "                                          self.state: state,\n",
    "                                          self.keep_probability: 1\n",
    "                                      })\n",
    "            epoch_cost += cost\n",
    "            epoch_iteration += self.time_steps\n",
    "        return self.perplexity(epoch_cost, epoch_iteration)\n",
    "\n",
    "    @staticmethod\n",
    "    def _interval(iteration, interval):\n",
    "        return interval is not None and iteration > 1 and iteration % interval == 0\n",
    "\n",
    "    @staticmethod\n",
    "    def perplexity(cost, iterations):\n",
    "        return np.exp(cost / iterations)\n",
    "\n",
    "    def store_validation_perplexity(self, session, summary, iteration, validation_perplexity):\n",
    "        session.run(self.validation_perplexity.assign(validation_perplexity))\n",
    "        summary.add_summary(session.run(self.summary), global_step=iteration)\n",
    "\n",
    "    def store_training_epoch_perplexity(self, session, summary, iteration, training_perplexity):\n",
    "        session.run(self.training_epoch_perplexity.assign(training_perplexity))\n",
    "        summary.add_summary(session.run(self.summary), global_step=iteration)\n",
    "\n",
    "    @staticmethod\n",
    "    def summary_writer(summary_directory, session):\n",
    "        class NullSummaryWriter(object):\n",
    "            def add_summary(self, *args, **kwargs):\n",
    "                pass\n",
    "\n",
    "            def flush(self):\n",
    "                pass\n",
    "\n",
    "            def close(self):\n",
    "                pass\n",
    "\n",
    "        if summary_directory is not None:\n",
    "            return tf.train.SummaryWriter(summary_directory, session.graph)\n",
    "        else:\n",
    "            return NullSummaryWriter()\n",
    "\n",
    "    @classmethod\n",
    "    def restore(cls, session, model_directory):\n",
    "        \"\"\"\n",
    "        Restore a previously trained model\n",
    "        :param session: session into which to restore the model\n",
    "        :type session: TensorFlow Session\n",
    "        :param model_directory: directory to which the model was saved\n",
    "        :type model_directory: str\n",
    "        :return: trained model\n",
    "        :rtype: RNN\n",
    "        \"\"\"\n",
    "        with open(cls._parameters_file(model_directory)) as f:\n",
    "            parameters = json.load(f)\n",
    "        model = cls(parameters[\"max_gradient\"],\n",
    "                    parameters[\"batch_size\"], parameters[\"time_steps\"], parameters[\"vocabulary_size\"],\n",
    "                    parameters[\"hidden_units\"], parameters[\"layers\"])\n",
    "        tf.train.Saver().restore(session, cls._model_file(model_directory))\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def _parameters_file(model_directory):\n",
    "        return os.path.join(model_directory, \"parameters.json\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _model_file(model_directory):\n",
    "        return os.path.join(model_directory, \"model\")\n",
    "\n",
    "class StopTrainingException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Objects used to group training parameters\n",
    "class ExitCriteria(object):\n",
    "    def __init__(self, max_iterations, max_epochs):\n",
    "        self.max_iterations = max_iterations\n",
    "        self.max_epochs = max_epochs\n",
    "\n",
    "\n",
    "class Parameters(object):\n",
    "    def __init__(self, learning_rate, keep_probability):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_probability = keep_probability\n",
    "\n",
    "\n",
    "class Validation(object):\n",
    "    def __init__(self, interval, validation_set):\n",
    "        self.interval = interval\n",
    "        self.validation_set = validation_set\n",
    "\n",
    "\n",
    "class Directories(object):\n",
    "    def __init__(self, model, summary):\n",
    "        self.model = model\n",
    "        self.summary = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_model(time_steps, rnn_layers, dense_layers=None):\n",
    "    \"\"\"\n",
    "    Creates a LSTM model based on:\n",
    "        * stacked lstm cells\n",
    "        * an optional dense layers\n",
    "    :param time_steps: the number of time steps the model will be looking at.\n",
    "    :param rnn_layers: list of int or dict\n",
    "                         * list of int: the steps used to instantiate the `BasicLSTMCell` cell\n",
    "                         * list of dict: [{steps: int, keep_prob: int}, ...]\n",
    "    :param dense_layers: list of nodes for each layer\n",
    "    :return: the model definition\n",
    "    \"\"\"\n",
    "\n",
    "    def lstm_cells(layers):\n",
    "        if isinstance(layers[0], dict):\n",
    "            return [tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.BasicLSTMCell(layer['steps'],\n",
    "                                                                               state_is_tuple=True),\n",
    "                                                  layer['keep_prob'])\n",
    "                    if layer.get('keep_prob') else tf.nn.rnn_cell.BasicLSTMCell(layer['steps'],\n",
    "                                                                                state_is_tuple=True)\n",
    "                    for layer in layers]\n",
    "        return [tf.nn.rnn_cell.BasicLSTMCell(steps, state_is_tuple=True) for steps in layers]\n",
    "\n",
    "    def dnn_layers(input_layers, layers):\n",
    "        if layers and isinstance(layers, dict):\n",
    "            return learn.ops.dnn(input_layers,\n",
    "                                 layers['layers'],\n",
    "                                 activation=layers.get('activation'),\n",
    "                                 dropout=layers.get('dropout'))\n",
    "        elif layers:\n",
    "            return learn.ops.dnn(input_layers, layers)\n",
    "        else:\n",
    "            return input_layers\n",
    "\n",
    "    def _lstm_model(X, y):\n",
    "        stacked_lstm = tf.nn.rnn_cell.MultiRNNCell(lstm_cells(rnn_layers), state_is_tuple=True)\n",
    "        x_ = learn.ops.split_squeeze(1, time_steps, X)\n",
    "        output, layers = tf.nn.rnn(stacked_lstm, x_, dtype=dtypes.float32)\n",
    "        output = dnn_layers(output[-1], dense_layers)\n",
    "        return learn.models.linear_regression(output, y)\n",
    "\n",
    "    return _lstm_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
