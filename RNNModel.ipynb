{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "from loadData import loadData, convertToBatches\n",
    "inputDir = \"GEFCom2012/\"\n",
    "import logging\n",
    "__version__ = \"1.0.0\"\n",
    "logging.basicConfig(filename='example.log',level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "    \"\"\"Recursive Neural Network 2 layers without CNN as feature extractor\"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, maxGradient, timeSteps, nHorizons, inputSize, nHiddenUnits, nLayers):\n",
    "        self.maxGradient = maxGradient\n",
    "        self.nLayers = nLayers\n",
    "        self.timeSteps = timeSteps\n",
    "        self.nHorizons = nHorizons\n",
    "        self.inputSize = inputSize\n",
    "        self.nHiddenUnits = nHiddenUnits\n",
    "        \n",
    "        with tf.name_scope(\"Parameters\"):\n",
    "            self.learningRate = tf.placeholder(tf.float32, name=\"learningRate\")\n",
    "            self.keepProbability = tf.placeholder(tf.float32, name=\"keepProbability\")\n",
    "            \n",
    "        with tf.name_scope(\"Input\"):\n",
    "            self.input = tf.placeholder(tf.float32, shape=(None, timeSteps, inputSize), name=\"input\")\n",
    "            self.targets = tf.placeholder(tf.float32, shape=(None, timeSteps, nHorizons), name=\"targets\")\n",
    "            self.init = tf.placeholder(tf.float32, shape=(), name=\"init\")\n",
    "            self.batchSize = self.input.get_shape()[0]\n",
    "        #Declare the CNN structure here!\n",
    "        #with tf.name_scope(\"Embedding\"):\n",
    "        #    self.embedding = tf.Variable(tf.random_uniform((inputSize, hidden_units), -self.init, self.init),\n",
    "        #                                 dtype=tf.float32,\n",
    "        #                                 name=\"embedding\")\n",
    "        #    self.w = tf.get_variable(\"w\", (inputSize, hidden_units))\n",
    "        #    self.b = tf.get_variable(\"b\", inputSize)\n",
    "            \n",
    "        #    self.embedded_input = tf.matmul(self.input, self.w) + self.b\n",
    "\n",
    "        with tf.name_scope(\"RNN\"):\n",
    "            cell = tf.nn.rnn_cell.LSTMCell(nHiddenUnits, state_is_tuple=True)\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=self.keepProbability)\n",
    "            self.rnn_layers = tf.nn.rnn_cell.MultiRNNCell([cell] * nLayers, state_is_tuple=True)\n",
    "            state_placeholder = tf.placeholder(tf.float32, [nLayers, 2, None, nHiddenUnits])\n",
    "            #Unpack the state_placeholder into tuple to use with tensorflow native RNN API\n",
    "            l = tf.unpack(state_placeholder, axis=0)\n",
    "            self.state = tuple(\n",
    "                                [tf.nn.rnn_cell.LSTMStateTuple(l[idx][0], l[idx][1]) \n",
    "                                for idx in range(nLayers)]\n",
    "                              )\n",
    "            \n",
    "            #print(self.reset_state)\n",
    "            #self.state = (tf.placeholder(tf.float32, shape=(batchSize, nHidden) , \"state\"))\n",
    "            self.outputs, self.nextState = tf.nn.dynamic_rnn(self.rnn_layers, self.input, time_major=False,\n",
    "                                                              initial_state=self.state)\n",
    "\n",
    "        with tf.name_scope(\"Cost\"):\n",
    "            # Concatenate all the batches into a single row.\n",
    "            self.flattenedOutputs = tf.reshape(self.outputs, (-1, nHiddenUnits),\n",
    "                                                name=\"flattenedOutputs\")\n",
    "            # Project the outputs onto the vocabulary.\n",
    "            self.w = tf.get_variable(\"w\", (nHiddenUnits, nHorizons))\n",
    "            self.b = tf.get_variable(\"b\", nHorizons)\n",
    "            self.predicted = tf.matmul(self.flattenedOutputs, self.w) + self.b\n",
    "            self.flattenedTargets = tf.reshape(self.targets, (-1, nHorizons), name = \"flattenedTargets\")\n",
    "            # Compare predictions to labels.\n",
    "            self.loss = tf.sqrt(tf.reduce_mean(tf.square(tf.sub(self.flattenedTargets, self.predicted))))\n",
    "            self.cost = tf.reduce_mean(self.loss, name=\"cost\")\n",
    "\n",
    "        with tf.name_scope(\"Train\"):\n",
    "            #self.validation_perplexity = tf.Variable(dtype=tf.float32, initial_value=float(\"inf\"), trainable=False,\n",
    "            #                                         name=\"validation_perplexity\")\n",
    "            tf.scalar_summary('RMSE', self.cost)\n",
    "            #self.training_epoch_perplexity = tf.Variable(dtype=tf.float32, initial_value=float(\"inf\"), trainable=False,\n",
    "            #                                             name=\"training_epoch_perplexity\")\n",
    "            #tf.scalar_summary(self.training_epoch_perplexity.op.name, self.training_epoch_perplexity)\n",
    "            self.iteration = tf.Variable(0, dtype=tf.int64, name=\"iteration\", trainable=False)\n",
    "            self.gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()),\n",
    "                                                       maxGradient, name=\"clipGradients\")\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learningRate)\n",
    "            self.trainStep = optimizer.apply_gradients(zip(self.gradients, tf.trainable_variables()),\n",
    "                                                        name=\"trainStep\",\n",
    "                                                        global_step=self.iteration)\n",
    "\n",
    "        self.initialize = tf.initialize_all_variables()\n",
    "        self.summary = tf.merge_all_summaries()\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self.input.get_shape()[0].value\n",
    "\n",
    "    @property\n",
    "    def time_steps(self):\n",
    "        return self.input.get_shape()[1].value\n",
    "\n",
    "    @property\n",
    "    def input_size(self):\n",
    "        return self.embedding.get_shape()[0].value\n",
    "\n",
    "    @property\n",
    "    def hidden_units(self):\n",
    "        return self.embedding.get_shape()[1].value\n",
    "\n",
    "    def train(self, session, init, ts, parameters, exitCriteria, validation, loggingInterval, directories):\n",
    "        epoch = 1\n",
    "        iteration = 0\n",
    "        state = None\n",
    "        trainingSet = convertToBatches(ts, self.timeSteps, parameters.batchSize, self.nHorizons)\n",
    "        self.resetState = self.rnn_layers.zero_state(parameters.batchSize, dtype=tf.float32)\n",
    "\n",
    "        summaryWriter = self.summaryWriter(directories.summary, session)\n",
    "        session.run(self.initialize, feed_dict={self.init: init})\n",
    "        try:\n",
    "            # Enumerate over the training set until exit criteria are met.\n",
    "            tsFit = []\n",
    "            tsTarget = []\n",
    "            lastState = None\n",
    "            while True:\n",
    "                if (exitCriteria.maxEpochs is not None) and (epoch > exitCriteria.maxEpochs):\n",
    "                    lastState = state\n",
    "                    raise StopTrainingException()\n",
    "                epochCost = epochIteration = 0\n",
    "                #Reset state after every epoch\n",
    "                state = session.run(self.resetState)\n",
    "                # Enumerate over a single epoch of the training set.\n",
    "                for xs, ys in trainingSet:\n",
    "                    _, summary, cost, state, iteration, predicted = session.run(\n",
    "                        [self.trainStep, self.summary, self.cost, self.nextState, self.iteration, self.predicted],\n",
    "                        feed_dict={\n",
    "                            self.input: xs,\n",
    "                            self.targets: ys,\n",
    "                            self.state: state,\n",
    "                            self.learningRate: parameters.learningRate,\n",
    "                            self.keepProbability: parameters.keepProbability\n",
    "                        })\n",
    "                    if (epoch == exitCriteria.maxEpochs):\n",
    "                        tsFit.append(predicted)\n",
    "                        tsTarget.append(ys)\n",
    "                        \n",
    "                    epochCost += cost\n",
    "                    epochIteration += self.timeSteps\n",
    "                    if self._interval(iteration, loggingInterval):\n",
    "                        logger.info(\"Epoch %d, Iteration %d: training loss %0.4f\" %\n",
    "                                (epoch, iteration, cost))\n",
    "                    #if validation is not None and self._interval(iteration, validation.interval):\n",
    "                    #    validation_perplexity = self.test(session, validation.validation_set)\n",
    "                        #self.store_validation_perplexity(session, summary, iteration, validation_perplexity)\n",
    "                        #self.store_rmse(session, summary, iteration, self.rmse)\n",
    "                    summaryWriter.add_summary(summary, iteration)\n",
    "                    \n",
    "\n",
    "                #self.store_trainingEpochRMSE(session, summary, iteration, epoch_cost)\n",
    "                logger.info(\"---Epoch %d, Iteration %d: epoch loss %0.4f\" % (epoch, iteration, epochCost))\n",
    "\n",
    "                epoch += 1\n",
    "                if (exitCriteria.maxIterations is not None) and (iteration > exitCriteria.maxIterations):\n",
    "                    raise StopTrainingException()\n",
    "        except (StopTrainingException, KeyboardInterrupt):\n",
    "            pass\n",
    "        logger.info(\"Stop training at epoch %d, iteration %d\" % (epoch, iteration))\n",
    "        summaryWriter.close()\n",
    "        if directories.model is not None:\n",
    "            modelFileName = self._modelFile(directories.model)\n",
    "            tf.train.Saver().save(session, modelFileName)\n",
    "            self._writeModelParameters(directories.model)\n",
    "            logger.info(\"Saved model in %s \" % directories.model)\n",
    "            \n",
    "        tsFit = np.reshape(np.asarray(tsFit), (-1, nHorizons))\n",
    "        tsTarget = np.reshape(np.asarray(tsTarget), (-1, nHorizons))\n",
    "        return (tsTarget, tsFit, lastState)\n",
    "\n",
    "    def _writeModelParameters(self, modelDirectory):\n",
    "        parameters = {\n",
    "            \"maxGradient\": self.maxGradient,\n",
    "            \"timeSteps\": self.timeSteps,\n",
    "            \"inputSize\": self.inputSize,\n",
    "            \"nHiddenUnits\": self.nHiddenUnits,\n",
    "            \"nLayers\": self.nLayers,\n",
    "            \"nHorizons\": self.nHorizons\n",
    "        }\n",
    "        with open(self._parametersFile(modelDirectory), \"w\") as f:\n",
    "            json.dump(parameters, f, indent=4)\n",
    "\n",
    "    def predict(self, session, startState, tsTest, batchSize):\n",
    "        state = None\n",
    "        testSet = convertToBatches(tsTest, self.timeSteps, batchSize, self.nHorizons, cutHead=False)\n",
    "        tsPredicted = []\n",
    "        tsTarget = []\n",
    "        epoch_cost = 0\n",
    "        state = startState\n",
    "        for xs, ys in testSet:\n",
    "            cost, state, predicted = session.run(\n",
    "                [self.cost, self.nextState, self.predicted],\n",
    "                feed_dict={\n",
    "                    self.input: xs,\n",
    "                    self.targets: ys,\n",
    "                    self.state: state,\n",
    "                    self.keepProbability: 1\n",
    "                })\n",
    "            epoch_cost += cost\n",
    "            tsPredicted.append(predicted)\n",
    "            tsTarget.append(ys)\n",
    "        \n",
    "        tsPredicted = np.reshape(np.asarray(tsPredicted), (-1, self.nHorizons))\n",
    "        tsTarget = np.reshape(np.asarray(tsTarget), (-1, self.nHorizons))\n",
    "        return (tsTarget, tsPredicted, epoch_cost)\n",
    "\n",
    "    @staticmethod\n",
    "    def _interval(iteration, interval):\n",
    "        return interval is not None and iteration > 1 and iteration % interval == 0\n",
    "\n",
    "    @staticmethod\n",
    "    def perplexity(cost, iterations):\n",
    "        return np.exp(cost / iterations)\n",
    "\n",
    "    def store_validation_perplexity(self, session, summary, iteration, validation_perplexity):\n",
    "        session.run(self.validation_perplexity.assign(validation_perplexity))\n",
    "        summary.addSummary(session.run(self.summary), global_step=iteration)\n",
    "\n",
    "    def store_training_epoch_perplexity(self, session, summary, iteration, training_perplexity):\n",
    "        session.run(self.training_epoch_perplexity.assign(training_perplexity))\n",
    "        summary.addSummary(session.run(self.summary), global_step=iteration)\n",
    "\n",
    "    @staticmethod\n",
    "    def summaryWriter(summaryDirectory, session):\n",
    "        class NullSummaryWriter(object):\n",
    "            def addSummary(self, *args, **kwargs):\n",
    "                pass\n",
    "\n",
    "            def flush(self):\n",
    "                pass\n",
    "\n",
    "            def close(self):\n",
    "                pass\n",
    "\n",
    "        if summaryDirectory is not None:\n",
    "            return tf.train.SummaryWriter(summaryDirectory, session.graph)\n",
    "        else:\n",
    "            return NullSummaryWriter()\n",
    "\n",
    "    @classmethod\n",
    "    def restore(cls, session, modelDirectory):\n",
    "        \"\"\"\n",
    "        Restore a previously trained model\n",
    "        :param session: session into which to restore the model\n",
    "        :type session: TensorFlow Session\n",
    "        :param model_directory: directory to which the model was saved\n",
    "        :type model_directory: str\n",
    "        :return: trained model\n",
    "        :rtype: RNN\n",
    "        \"\"\"\n",
    "        with open(cls._parametersFile(modelDirectory)) as f:\n",
    "            parameters = json.load(f)\n",
    "        model = cls(parameters[\"maxGradient\"], parameters[\"timeSteps\"], parameters[\"nHorizons\"], \n",
    "                    parameters[\"inputSize\"],parameters[\"nHiddenUnits\"], parameters[\"nLayers\"])\n",
    "        tf.train.Saver().restore(session, cls._modelFile(modelDirectory))\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def _parametersFile(modelDirectory):\n",
    "        return os.path.join(modelDirectory, \"parameters.json\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _modelFile(modelDirectory):\n",
    "        return os.path.join(modelDirectory, \"model\")\n",
    "\n",
    "class StopTrainingException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Objects used to group training parameters\n",
    "class ExitCriteria(object):\n",
    "    def __init__(self, maxIterations, maxEpochs):\n",
    "        self.maxIterations = maxIterations\n",
    "        self.maxEpochs = maxEpochs\n",
    "\n",
    "\n",
    "class Parameters(object):\n",
    "    def __init__(self, learningRate, keepProbability, batchSize):\n",
    "        self.learningRate = learningRate\n",
    "        self.keepProbability = keepProbability\n",
    "        self.batchSize = batchSize\n",
    "\n",
    "\n",
    "class Validation(object):\n",
    "    def __init__(self, interval, validation_set):\n",
    "        self.interval = interval\n",
    "        self.validation_set = validation_set\n",
    "\n",
    "\n",
    "class Directories(object):\n",
    "    def __init__(self, model, summary):\n",
    "        self.model = model\n",
    "        self.summary = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelDirectory = \"GEFCom2012/Model\"\n",
    "summaryDirectory = \"GEFCom2012/Model\"\n",
    "maxGradient = 1\n",
    "timeSteps = 24\n",
    "nHorizons = 24\n",
    "batchSize = 1\n",
    "nHiddenUnits = 200\n",
    "nLayers = 2\n",
    "init = 0.05\n",
    "inputSize = 1\n",
    "keepProbability = 0.5\n",
    "maxEpochs=200\n",
    "learningRate=1.0\n",
    "maxIterations = 1000000\n",
    "loggingInterval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train\n",
    "trainingDfs, completeDfs = loadData(\"GEFCom2012/\", maxDataPoints = -1)\n",
    "ts = trainingDfs[0][[\"zone.1\"]].values\n",
    "batches = convertToBatches(ts, timeSteps, batchSize, nHorizons)\n",
    "\n",
    "\n",
    "ts = trainingDfs[0][[\"zone.1\"]].values\n",
    "tsTest = completeDfs[0][[\"zone.1\"]].values\n",
    "tsTest = tsTest[len(ts):]\n",
    "##Normalize ts\n",
    "tsMean = ts.mean()\n",
    "tsRange = ts.max()-ts.min()\n",
    "ts = (ts-tsMean)/tsRange\n",
    "tsTest = (tsTest-tsMean)/tsRange\n",
    "if modelDirectory is None:\n",
    "    logger.warn(\"Not saving a model.\")\n",
    "logger.info(\"Start Experiment\")\n",
    "\n",
    "# Run training.\n",
    "start_time = time.time()\n",
    "with tf.device('/gpu:0'):\n",
    "    with tf.Graph().as_default():\n",
    "        model = RNN(maxGradient = maxGradient, \n",
    "                    timeSteps = timeSteps, \n",
    "                    nHorizons = nHorizons, \n",
    "                    inputSize = inputSize, \n",
    "                    nHiddenUnits = nHiddenUnits, \n",
    "                    nLayers = nLayers)\n",
    "        with tf.Session() as session:\n",
    "            tsTarget, tsFit, lastState = model.train(session = session, \n",
    "                        init = init, \n",
    "                        ts = ts, \n",
    "                        parameters = Parameters(learningRate, keepProbability, batchSize), \n",
    "                        exitCriteria = ExitCriteria(maxIterations, maxEpochs), \n",
    "                        validation = None, \n",
    "                        loggingInterval = loggingInterval, \n",
    "                        directories = Directories(modelDirectory, summaryDirectory))\n",
    "logger.info(\"Total training time %s\" % timedelta(seconds=(time.time() - start_time)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lastState' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-01f085086431>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelDirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             tsTarget, tsPredicted, epoch_cost = model.predict(session = session,\n\u001b[0;32m----> 7\u001b[0;31m                                                               \u001b[0mstartState\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlastState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                                                               \u001b[0mtsTest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsTest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                               batchSize = 1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lastState' is not defined"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "with tf.device('/gpu:0'):\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session() as session:\n",
    "            model = RNN.restore(session, modelDirectory)\n",
    "            tsTarget, tsPredicted, epoch_cost = model.predict(session = session,\n",
    "                                                              startState = lastState, \n",
    "                                                              tsTest = tsTest,\n",
    "                                                              batchSize = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "print np.sqrt(np.mean(np.square(tsTarget[:, 23]-tsPredicted[:, 23])))\n",
    "x = pd.Series(tsPredicted[:, 23])\n",
    "y = pd.Series(tsTarget[:, 23])\n",
    "x.plot(title=\"Fited\")\n",
    "plt.figure()\n",
    "y.plot(title=\"True\")\n",
    "\n",
    "x = x*tsRange+tsMean\n",
    "y = y*tsRange+tsMean\n",
    "np.mean(np.abs(x-y)/y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testSet = convertToBatches(tsTest, timeSteps, batchSize, nHorizons, cutHead=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = len(tsTest)\n",
    "m = timeSteps * batchSize\n",
    "p = (n-nHorizons) % m\n",
    "#Remove first part of ts to make its length n multiple of timeSteps*batchSize+1\n",
    "\n",
    "if (p==0):\n",
    "    cleanTs = tsTest\n",
    "else:\n",
    "    if False:\n",
    "        cleanTs = tsTest[p:]\n",
    "    else:\n",
    "        cleanTs = tsTest[:-p]\n",
    "n = len(cleanTs)\n",
    "instances = (n-nHorizons)/m\n",
    "xs = cleanTs[:-nHorizons].reshape(instances, batchSize, timeSteps, 1) #shape [nInstances x batchSize x timeSteps x 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    cleanTs = tsTest[p:]\n",
    "else:\n",
    "    cleanTs = tsTest[:-p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleanTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
