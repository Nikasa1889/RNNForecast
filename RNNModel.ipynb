{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "from loadData import loadData, convertToBatches\n",
    "inputDir = \"GEFCom2012/\"\n",
    "import logging\n",
    "__version__ = \"1.0.0\"\n",
    "logging.basicConfig(filename='example.log',level=logging.DEBUG)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "nHorizons = 24\n",
    "timeSteps = 20\n",
    "batchSize = 20\n",
    "\n",
    "trainingDfs, completeDfs = loadData(\"GEFCom2012/\", maxDataPoints = -1)\n",
    "ts = trainingDfs[0][[\"zone.1\"]].values\n",
    "batches = convertToBatches(ts, timeSteps, batchSize, nHorizons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "    \"\"\"Recursive Neural Network\"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, maxGradient, batchSize, timeSteps, nHorizons, inputSize, nHiddenUnits, nLayers):\n",
    "        self.maxGradient = maxGradient\n",
    "        self.nLayers = nLayers\n",
    "        self.timeSteps = timeSteps\n",
    "        self.nHorizons = nHorizons\n",
    "        self.inputSize = inputSize\n",
    "        self.batchSize = batchSize\n",
    "        self.nHiddenUnits = nHiddenUnits\n",
    "        \n",
    "        with tf.name_scope(\"Parameters\"):\n",
    "            self.learningRate = tf.placeholder(tf.float32, name=\"learningRate\")\n",
    "            self.keepProbability = tf.placeholder(tf.float32, name=\"keepProbability\")\n",
    "\n",
    "        with tf.name_scope(\"Input\"):\n",
    "            self.input = tf.placeholder(tf.float32, shape=(batchSize, timeSteps, inputSize), name=\"input\")\n",
    "            self.targets = tf.placeholder(tf.float32, shape=(batchSize, timeSteps, nHorizons), name=\"targets\")\n",
    "            self.init = tf.placeholder(tf.float32, shape=(), name=\"init\")\n",
    "        #Declare the CNN structure here!\n",
    "        #with tf.name_scope(\"Embedding\"):\n",
    "        #    self.embedding = tf.Variable(tf.random_uniform((inputSize, hidden_units), -self.init, self.init),\n",
    "        #                                 dtype=tf.float32,\n",
    "        #                                 name=\"embedding\")\n",
    "        #    self.w = tf.get_variable(\"w\", (inputSize, hidden_units))\n",
    "        #    self.b = tf.get_variable(\"b\", inputSize)\n",
    "            \n",
    "        #    self.embedded_input = tf.matmul(self.input, self.w) + self.b\n",
    "\n",
    "        with tf.name_scope(\"RNN\"):\n",
    "            cell = tf.nn.rnn_cell.LSTMCell(nHiddenUnits, state_is_tuple=True)\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=self.keepProbability)\n",
    "            rnn_layers = tf.nn.rnn_cell.MultiRNNCell([cell] * nLayers, state_is_tuple=True)\n",
    "            self.resetState = rnn_layers.zero_state(batchSize, dtype=tf.float32)\n",
    "            state_placeholder = tf.placeholder(tf.float32, [nLayers, 2, batchSize, nHiddenUnits])\n",
    "            #Unpack the state_placeholder into tuple to use with tensorflow native RNN API\n",
    "            l = tf.unpack(state_placeholder, axis=0)\n",
    "            self.state = tuple(\n",
    "                                [tf.nn.rnn_cell.LSTMStateTuple(l[idx][0], l[idx][1]) \n",
    "                                for idx in range(nLayers)]\n",
    "                              )\n",
    "            \n",
    "            #print(self.reset_state)\n",
    "            #self.state = (tf.placeholder(tf.float32, shape=(batchSize, nHidden) , \"state\"))\n",
    "            self.outputs, self.nextState = tf.nn.dynamic_rnn(rnn_layers, self.input, time_major=True,\n",
    "                                                              initial_state=self.state)\n",
    "\n",
    "        with tf.name_scope(\"Cost\"):\n",
    "            # Concatenate all the batches into a single row.\n",
    "            self.flattenedOutputs = tf.reshape(self.outputs, (-1, nHiddenUnits),\n",
    "                                                name=\"flattenedOutputs\")\n",
    "            # Project the outputs onto the vocabulary.\n",
    "            self.w = tf.get_variable(\"w\", (nHiddenUnits, nHorizons))\n",
    "            self.b = tf.get_variable(\"b\", nHorizons)\n",
    "            self.predicted = tf.matmul(self.flattenedOutputs, self.w) + self.b\n",
    "            self.flattenedTargets = tf.reshape(self.targets, (-1, nHorizons), name = \"flattenedTargets\")\n",
    "            # Compare predictions to labels.\n",
    "            self.loss = tf.sqrt(tf.reduce_mean(tf.square(tf.sub(self.flattenedTargets, self.predicted))))\n",
    "            self.cost = tf.div(tf.reduce_sum(self.loss), batchSize, name=\"cost\")\n",
    "\n",
    "        with tf.name_scope(\"Train\"):\n",
    "            #self.validation_perplexity = tf.Variable(dtype=tf.float32, initial_value=float(\"inf\"), trainable=False,\n",
    "            #                                         name=\"validation_perplexity\")\n",
    "            #tf.scalar_summary(self.validation_perplexity.op.name, self.validation_perplexity)\n",
    "            #self.training_epoch_perplexity = tf.Variable(dtype=tf.float32, initial_value=float(\"inf\"), trainable=False,\n",
    "            #                                             name=\"training_epoch_perplexity\")\n",
    "            #tf.scalar_summary(self.training_epoch_perplexity.op.name, self.training_epoch_perplexity)\n",
    "            self.iteration = tf.Variable(0, dtype=tf.int64, name=\"iteration\", trainable=False)\n",
    "            self.gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()),\n",
    "                                                       maxGradient, name=\"clipGradients\")\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learningRate)\n",
    "            self.trainStep = optimizer.apply_gradients(zip(self.gradients, tf.trainable_variables()),\n",
    "                                                        name=\"trainStep\",\n",
    "                                                        global_step=self.iteration)\n",
    "\n",
    "        self.initialize = tf.initialize_all_variables()\n",
    "        self.summary = tf.merge_all_summaries()\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self.input.get_shape()[0].value\n",
    "\n",
    "    @property\n",
    "    def time_steps(self):\n",
    "        return self.input.get_shape()[1].value\n",
    "\n",
    "    @property\n",
    "    def vocabulary_size(self):\n",
    "        return self.embedding.get_shape()[0].value\n",
    "\n",
    "    @property\n",
    "    def hidden_units(self):\n",
    "        return self.embedding.get_shape()[1].value\n",
    "\n",
    "    def train(self, session, init, ts, parameters, exitCriteria, validation, loggingInterval, directories):\n",
    "        epoch = 1\n",
    "        iteration = 0\n",
    "        state = None\n",
    "        trainingSet = convertToBatches(ts, self.timeSteps, self.batchSize, self.nHorizons)\n",
    "        summary = self.summaryWriter(directories.summary, session)\n",
    "        session.run(self.initialize, feed_dict={self.init: init})\n",
    "        try:\n",
    "            # Enumerate over the training set until exit criteria are met.\n",
    "            while True:\n",
    "                epochCost = epochIteration = 0\n",
    "                #Reset state after every epoch\n",
    "                state = session.run(self.resetState)\n",
    "                # Enumerate over a single epoch of the training set.\n",
    "                for xs, ys in trainingSet:\n",
    "                    _, cost, state, iteration = session.run(\n",
    "                        [self.trainStep, self.cost, self.nextState, self.iteration],\n",
    "                        feed_dict={\n",
    "                            self.input: xs,\n",
    "                            self.targets: ys,\n",
    "                            self.state: state,\n",
    "                            self.learningRate: parameters.learningRate,\n",
    "                            self.keepProbability: parameters.keepProbability\n",
    "                        })\n",
    "                    epochCost += cost\n",
    "                    epochIteration += self.timeSteps\n",
    "                    #if self._interval(iteration, loggingInterval):\n",
    "                        #logger.info(\"Epoch %d, Iteration %d: epoch training perplexity %0.4f\" %\n",
    "                    #                (epoch, iteration, self.perplexity(epochCost, epochIteration)))\n",
    "                    #if validation is not None and self._interval(iteration, validation.interval):\n",
    "                    #    validation_perplexity = self.test(session, validation.validation_set)\n",
    "                        #self.store_validation_perplexity(session, summary, iteration, validation_perplexity)\n",
    "                        #self.store_rmse(session, summary, iteration, self.rmse)\n",
    "                    logger.info(\"Epoch %d, Iteration %d: training loss %0.4f\" %\n",
    "                                (epoch, iteration, cost))\n",
    "                    if (exitCriteria.maxIterations is not None) and (iteration > exitCriteria.maxIterations):\n",
    "                        raise StopTrainingException()\n",
    "\n",
    "                #self.store_trainingEpochRMSE(session, summary, iteration, epoch_cost)\n",
    "                logger.info(\"---Epoch %d, Iteration %d: epoch loss %0.4f\" % (epoch, iteration, epochCost))\n",
    "\n",
    "                epoch += 1\n",
    "                if (exitCriteria.maxIterations is not None) and (iteration > exitCriteria.maxIterations):\n",
    "                    raise StopTrainingException()\n",
    "        except (StopTrainingException, KeyboardInterrupt):\n",
    "            pass\n",
    "        logger.info(\"Stop training at epoch %d, iteration %d\" % (epoch, iteration))\n",
    "        summary.close()\n",
    "        if directories.model is not None:\n",
    "            modelFileName = self._modelFile(directories.model)\n",
    "            tf.train.Saver().save(session, modelFileName)\n",
    "            self._writeModelParameters(directories.model)\n",
    "            logger.info(\"Saved model in %s \" % directories.model)\n",
    "\n",
    "    def _writeModelParameters(self, modelDirectory):\n",
    "        parameters = {\n",
    "            \"maxGradient\": self.maxGradient,\n",
    "            \"batchSize\": self.batchSize,\n",
    "            \"timeSteps\": self.timeSteps,\n",
    "            \"inputSize\": self.inputSize,\n",
    "            \"nHiddenUnits\": self.nHiddenUnits,\n",
    "            \"nLayers\": self.nLayers\n",
    "        }\n",
    "        with open(self._parametersFile(modelDirectory), \"w\") as f:\n",
    "            json.dump(parameters, f, indent=4)\n",
    "\n",
    "    def test(self, session, test_set):\n",
    "        state = None\n",
    "        epoch_cost = epoch_iteration = 0\n",
    "        for start_document, context, target, _ in test_set.epoch(self.time_steps, self.batch_size):\n",
    "            if start_document:\n",
    "                state = session.run(self.reset_state)\n",
    "            cost, state = session.run([self.cost, self.nextState],\n",
    "                                      feed_dict={\n",
    "                                          self.input: context,\n",
    "                                          self.targets: target,\n",
    "                                          self.state: state,\n",
    "                                          self.keepProbability: 1\n",
    "                                      })\n",
    "            epoch_cost += cost\n",
    "            epoch_iteration += self.time_steps\n",
    "        return self.perplexity(epoch_cost, epoch_iteration)\n",
    "\n",
    "    @staticmethod\n",
    "    def _interval(iteration, interval):\n",
    "        return interval is not None and iteration > 1 and iteration % interval == 0\n",
    "\n",
    "    @staticmethod\n",
    "    def perplexity(cost, iterations):\n",
    "        return np.exp(cost / iterations)\n",
    "\n",
    "    def store_validation_perplexity(self, session, summary, iteration, validation_perplexity):\n",
    "        session.run(self.validation_perplexity.assign(validation_perplexity))\n",
    "        summary.addSummary(session.run(self.summary), global_step=iteration)\n",
    "\n",
    "    def store_training_epoch_perplexity(self, session, summary, iteration, training_perplexity):\n",
    "        session.run(self.training_epoch_perplexity.assign(training_perplexity))\n",
    "        summary.addSummary(session.run(self.summary), global_step=iteration)\n",
    "\n",
    "    @staticmethod\n",
    "    def summaryWriter(summaryDirectory, session):\n",
    "        class NullSummaryWriter(object):\n",
    "            def addSummary(self, *args, **kwargs):\n",
    "                pass\n",
    "\n",
    "            def flush(self):\n",
    "                pass\n",
    "\n",
    "            def close(self):\n",
    "                pass\n",
    "\n",
    "        if summaryDirectory is not None:\n",
    "            return tf.train.SummaryWriter(summaryDirectory, session.graph)\n",
    "        else:\n",
    "            return NullSummaryWriter()\n",
    "\n",
    "    @classmethod\n",
    "    def restore(cls, session, modelDirectory):\n",
    "        \"\"\"\n",
    "        Restore a previously trained model\n",
    "        :param session: session into which to restore the model\n",
    "        :type session: TensorFlow Session\n",
    "        :param model_directory: directory to which the model was saved\n",
    "        :type model_directory: str\n",
    "        :return: trained model\n",
    "        :rtype: RNN\n",
    "        \"\"\"\n",
    "        with open(cls._parametersFile(modelDirectory)) as f:\n",
    "            parameters = json.load(f)\n",
    "        model = cls(parameters[\"maxGradient\"],\n",
    "                    parameters[\"batchSize\"], parameters[\"timeSteps\"], parameters[\"inputSize\"],\n",
    "                    parameters[\"hiddenUnits\"], parameters[\"nLayers\"])\n",
    "        tf.train.Saver().restore(session, cls._modelFile(modelDirectory))\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def _parametersFile(modelDirectory):\n",
    "        return os.path.join(modelDirectory, \"parameters.json\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _modelFile(modelDirectory):\n",
    "        return os.path.join(modelDirectory, \"model\")\n",
    "\n",
    "class StopTrainingException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Objects used to group training parameters\n",
    "class ExitCriteria(object):\n",
    "    def __init__(self, maxIterations, maxEpochs):\n",
    "        self.maxIterations = maxIterations\n",
    "        self.maxEpochs = maxEpochs\n",
    "\n",
    "\n",
    "class Parameters(object):\n",
    "    def __init__(self, learningRate, keepProbability):\n",
    "        self.learningRate = learningRate\n",
    "        self.keepProbability = keepProbability\n",
    "\n",
    "\n",
    "class Validation(object):\n",
    "    def __init__(self, interval, validation_set):\n",
    "        self.interval = interval\n",
    "        self.validation_set = validation_set\n",
    "\n",
    "\n",
    "class Directories(object):\n",
    "    def __init__(self, model, summary):\n",
    "        self.model = model\n",
    "        self.summary = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelDirectory = \"/GEFCom2012/Model\"\n",
    "summaryDirectory = \"/GEFCom2012/Model\"\n",
    "maxGradient = 5\n",
    "batchSize = 20\n",
    "nHiddenUnits = 200\n",
    "nLayers = 2\n",
    "init = 0.05\n",
    "inputSize = 1\n",
    "keepProbability=1\n",
    "maxEpochs=100\n",
    "learningRate=1.0\n",
    "maxIterations = 71*100\n",
    "loggingInterval = 1\n",
    "\n",
    "ts = trainingDfs[0][[\"zone.1\"]].values\n",
    "##Normalize ts\n",
    "ts = (ts-ts.mean())/(ts.max()-ts.min())\n",
    "if modelDirectory is None:\n",
    "    logger.warn(\"Not saving a model.\")\n",
    "logger.info(\"Start Experiment\")\n",
    "\n",
    "# Run training.\n",
    "start_time = time.time()\n",
    "with tf.device('/gpu:0'):\n",
    "    with tf.Graph().as_default():\n",
    "        model = RNN(maxGradient = maxGradient, \n",
    "                    batchSize = batchSize, \n",
    "                    timeSteps = timeSteps, \n",
    "                    nHorizons = nHorizons, \n",
    "                    inputSize = inputSize, \n",
    "                    nHiddenUnits = nHiddenUnits, \n",
    "                    nLayers = nLayers)\n",
    "        with tf.Session() as session:\n",
    "            model.train(session = session, \n",
    "                        init = init, \n",
    "                        ts = ts, \n",
    "                        parameters = Parameters(learningRate, keepProbability), \n",
    "                        exitCriteria = ExitCriteria(maxIterations, maxEpochs), \n",
    "                        validation = None, \n",
    "                        loggingInterval = loggingInterval, \n",
    "                        directories = Directories(modelDirectory, summaryDirectory))\n",
    "logger.info(\"Total training time %s\" % timedelta(seconds=(time.time() - start_time)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
